import { functionDefinitions } from '@/lib/ai-agent/definitions';
import { GoogleGenAI, LiveServerMessage, MediaResolution, Modality, Session } from '@google/genai';
import { AudioProcessor } from './audio-utils';

interface RAGResult {
  report_id: string;
  location_id: string;
  location_name: string;
  address: string;
  city: string;
  province: string;
  country: string;
  grade: string;
  score: number;
  ai_description: string;
  lan: number;
  lat: number;
  img_url: string;
  type: string;
  similarity_score: number;
  text_similarity?: number;
  image_similarity?: number;
}

interface AIAgentConfig {
  onLocationDetails: (locationId: string, focusMap: boolean) => void;
  onNavigate: (locationId: string, transportMode: string) => void;
  onHighlightLocations: (locationIds: string[], highlightType: string) => void;
  onSetMapFilter: (filter: string) => void;
  onFindNearby: (coordinates?: [number, number], radiusKm?: number, filterType?: string) => void;
  onAudioGenerated: (audioData: string) => void;
  onTextGenerated?: (text: string) => void; // Add text callback
  userLocation?: [number, number];
}

export class AIAgentService {
  private session: Session | undefined;
  private responseQueue: LiveServerMessage[] = [];
  private config: AIAgentConfig;
  private isConnected: boolean = false;
  private isConnecting: boolean = false;
  private connectionAttempts: number = 0;
  private maxConnectionAttempts: number = 3;
  private isProcessing: boolean = false;
  private audioChunks: string[] = [];
  private isPlayingAudio: boolean = false;
  private pendingFunctionCalls: Array<{name: string, args: unknown, id: string}> = [];

  constructor(config: AIAgentConfig) {
    this.config = config;
  }

  async initialize() {
    if (this.isConnecting || this.isConnected) {
      console.log('AI Agent already connecting or connected');
      return;
    }

    this.isConnecting = true;
    this.connectionAttempts++;

    try {
      const ai = new GoogleGenAI({
        apiKey: process.env.NEXT_PUBLIC_GEMINI_API_KEY || '',
      });

      // FIXED: Use the exact model from documentation
      const model = 'models/gemini-2.5-flash-live-preview';
      const tools = [{ functionDeclarations: functionDefinitions }];

      // FIXED: Follow documentation pattern - only AUDIO modality for function calls
      const sessionConfig = {
        responseModalities: [Modality.AUDIO], // Back to AUDIO only like documentation
        mediaResolution: MediaResolution.MEDIA_RESOLUTION_MEDIUM,
        speechConfig: {
          languageCode: 'id-ID',
          voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Aoede' } }
        },
        tools,
        systemInstruction: `You are a voice assistant for Sampahin app. 

CRITICAL INSTRUCTIONS:
1. ALWAYS call appropriate functions based on user requests
2. ALWAYS provide audio response in Indonesian (3-5 seconds)
3. Function calls are MANDATORY - never skip them
4. When user asks for location details, navigation, filtering, etc., you MUST call the corresponding functions.

Use the exact location_id values from the provided RAG data when calling functions.`
      };

      console.log('üîß Initializing AI Agent with config:', sessionConfig);

      this.session = await ai.live.connect({
        model,
        callbacks: {
          onopen: () => {
            console.debug('‚úÖ AI Agent session opened');
            this.isConnected = true;
            this.isConnecting = false;
            this.connectionAttempts = 0;
            this.responseQueue = [];
          },
          onmessage: (message: LiveServerMessage) => {
            this.responseQueue.push(message);
          },
          onerror: (e: ErrorEvent) => {
            console.error('‚ùå AI Agent error:', e.message);
            this.isConnected = false;
            this.isConnecting = false;
            this.isProcessing = false;
          },
          onclose: (e: CloseEvent) => {
            console.debug('üîí AI Agent session closed:', e.reason);
            this.isConnected = false;
            this.isConnecting = false;
            this.isProcessing = false;
            this.session = undefined;
          },
        },
        config: sessionConfig
      });

      await this.waitForConnection();
    } catch (error) {
      console.error('‚ùå Failed to initialize AI Agent:', error);
      this.isConnecting = false;
      this.isProcessing = false;
      throw error;
    }
  }

  private async waitForConnection(timeout: number = 10000): Promise<void> {
    return new Promise((resolve, reject) => {
      const startTime = Date.now();
      const checkConnection = () => {
        if (this.isConnected) resolve();
        else if (Date.now() - startTime > timeout) reject(new Error('Connection timeout'));
        else setTimeout(checkConnection, 100);
      };
      checkConnection();
    });
  }

  private async ensureConnection(): Promise<boolean> {
    if (this.isConnected && this.session) return true;
    if (this.connectionAttempts >= this.maxConnectionAttempts) {
      console.error('Max connection attempts reached');
      return false;
    }
    try {
      await this.initialize();
      return this.isConnected;
    } catch (error) {
      console.error('Failed to reconnect:', error);
      return false;
    }
  }

  async sendMessage(text: string, imageBase64?: string, ragResults?: RAGResult[]) {
    if (this.isProcessing) {
      console.log('Already processing a message, skipping...');
      return;
    }

    const connectionReady = await this.ensureConnection();
    if (!connectionReady || !this.session) {
      throw new Error('AI Agent session not available');
    }

    this.isProcessing = true;
    this.responseQueue = [];
    this.audioChunks = [];

    try {
      const ragContext = this.buildRAGContext(ragResults);
      const systemPrompt = this.buildSystemPrompt(ragContext);
      const fullPrompt = `${systemPrompt}\n\nUser Query: ${text}`;

      console.log('üì§ Sending to AI Agent:', { 
        text, 
        hasImage: !!imageBase64, 
        ragResultsCount: ragResults?.length || 0,
        promptLength: fullPrompt.length
      });

      this.session.sendClientContent({ turns: [fullPrompt] });

      // FIXED: Use documentation pattern
      await this.handleTurn();

    } catch (error) {
      console.error('Error sending message:', error);
      this.isConnected = false;
      this.session = undefined;
      throw error;
    } finally {
      this.isProcessing = false;
    }
  }

  private buildRAGContext(ragResults?: RAGResult[]): string {
    if (!ragResults || ragResults.length === 0) return "Tidak ada data lokasi yang relevan ditemukan.";
    let context = `Data lokasi yang relevan berdasarkan pencarian:\n\n`;
    ragResults.forEach((result, index) => {
      context += `${index + 1}. **${result.location_name}**\n   - ID: ${result.location_id}\n   - Alamat: ${result.address}, ${result.city}\n   - Grade: ${result.grade} (Skor: ${result.score}/100)\n\n`;
    });
    return context;
  }


private buildSystemPrompt(ragContext: string): string {
  const userLocationInfo = this.config.userLocation 
    ? `Lokasi user saat ini: [${this.config.userLocation[0]}, ${this.config.userLocation[1]}]` 
    : "Lokasi user tidak diketahui";

  return `# Sampahin AI Assistant - Voice & Function Interface

## CRITICAL INSTRUCTIONS:
1. ALWAYS provide conversational audio response in Indonesian first
2. Call appropriate functions DURING or AFTER your explanation
3. Function calls can happen at any time during conversation, not just at the start
4. Explain what you're showing while calling the functions
5. LANGSUNG GUNAKAN DATA RAG - Jangan tanya tempat apa yang dimaksud, gunakan hasil similarity yang tinggi

## Your Identity:
Voice assistant untuk aplikasi Sampahin - monitoring kebersihan lingkungan Indonesia.

## Context:
${userLocationInfo}

## Available Data:
${ragContext}

## RESPONSE STRATEGY - NATURAL CONVERSATION:
1. **Start with conversational audio explanation**
2. **Call functions while or after explaining** 
3. **Continue explaining what the user will see**

## RAG DATA USAGE RULES:
- JIKA ada data RAG dengan similarity_score > 0.3, LANGSUNG gunakan data tersebut
- JANGAN tanya "tempat apa yang dimaksud" atau "lokasi mana yang Anda maksud"
- PRIORITAS: text_similarity dan image_similarity yang tinggi = jawaban yang tepat
- UNTUK GAMBAR: Similarity > 0.4 = PASTI benar, langsung gunakan
- UNTUK TEKS: Similarity > 0.5 = PASTI benar, langsung gunakan
- Jika ada multiple results, pilih yang similarity tertinggi

## FUNCTION CALL MAPPING:

### User wants location details:
- Keywords: "detail", "info", "tampilkan", "lihat", "buka", "informasi", "tunjukkan"
- Audio Response: "Baik, saya akan menampilkan detail lokasi [nama] untuk Anda. Lokasi ini berada di [alamat] dengan grade kebersihan [grade] dan skor [score] dari 100. [Penjelasan kondisi]"
- THEN CALL: show_location_details(location_id_from_RAG, focus_map: true)
- Continue: "Sekarang Anda bisa melihat detail lengkapnya di sidebar."

### User wants navigation:
- Keywords: "rute", "arah", "navigasi", "jalan", "pergi ke", "carikan jalan"
- Audio Response: "Saya akan membantu mencarikan rute ke [nama lokasi] di [alamat]. Lokasi ini memiliki kondisi kebersihan grade [grade]."
- THEN CALL: navigate_to_location(location_id_from_RAG, transport_mode: "driving")
- Continue: "Navigasi telah dimulai. Ikuti petunjuk di peta untuk sampai ke tujuan."

### User wants to see/highlight locations:
- Keywords: "tunjukkan", "sorot", "highlight", "tampilkan di peta", "yang ada di", "apa saja"
- Audio Response: "Saya akan menampilkan [jumlah] lokasi yang sesuai dengan pencarian Anda. Mari saya sorot lokasinya di peta."
- THEN CALL: highlight_locations([location_ids_from_RAG], highlight_type: "pulse")
- Continue: "Sekarang Anda bisa melihat lokasi-lokasi tersebut tersorot di peta dengan warna yang berbeda."

### For IMAGE queries - DIRECT RECOGNITION:
- JIKA user kirim gambar DAN ada hasil RAG dengan image_similarity > 0.4:
- Audio Response: "Saya dapat mengenali lokasi ini! Ini adalah [nama lokasi] di [alamat]. Berdasarkan gambar yang Anda kirim, lokasi ini memiliki grade [grade] dengan skor [score]. [Deskripsi kondisi]"
- THEN CALL: show_location_details(location_id_with_highest_similarity, focus_map: true)
- Continue: "Detail lengkap lokasi sudah ditampilkan di sidebar."

## ENHANCED EXAMPLES WITH NATURAL FLOW:

**Input**: Gambar lokasi + "ini dimana?"
**DON'T ASK**: "Bisa tolong jelaskan tempat apa yang dimaksud?"
**DO THIS**: 
Audio: "Saya dapat mengenali lokasi ini! Ini adalah Taman Kota Bandung di Jalan Asia Afrika. Berdasarkan gambar, kondisi kebersihan saat ini grade B dengan skor 75 dari 100. Tempat ini cukup bersih dengan beberapa area yang perlu perbaikan."
Function: show_location_details("BANDUNG_001", true)
Audio continues: "Detail lengkap sudah ditampilkan di sidebar untuk Anda."

**Input**: "Tampilkan detail lokasi Tasik"
**Response**:
Audio: "Baik, saya akan menampilkan detail Tasik untuk Anda. Lokasi ini berada di Tasikmalaya, Jawa Barat dengan grade kebersihan D dan skor 20 dari 100. Kondisi cukup kotor dan memerlukan pembersihan."
Function: show_location_details("TASIK_001", true)
Audio continues: "Sekarang sidebar detail telah terbuka dan Anda bisa melihat informasi lengkapnya."

**Input**: "Lokasi kotor yang ada di Tasik ada apa saja?"
**Response**:
Audio: "Mari saya cari lokasi kotor di area Tasikmalaya. Dari data yang tersedia, ada beberapa lokasi dengan kondisi kurang baik di area tersebut."
Function: highlight_locations(["TASIK_001", "TASIK_002"], "pulse")
Audio continues: "Saya telah menyorot 2 lokasi kotor di Tasik. Yang terkotor adalah [nama] dengan grade D. Lokasi-lokasi ini terlihat berkedip di peta."

## CRITICAL RULES:
- ALWAYS start with natural conversational explanation
- Call functions DURING your explanation, not before
- Continue explaining what the user will see after function call
- Use real data from RAG results in your audio response
- Be conversational and helpful, not robotic
- LANGSUNG GUNAKAN DATA RAG DENGAN SIMILARITY TINGGI - NO QUESTIONS!
- UNTUK GAMBAR: Confidence tinggi = langsung jawab dengan detail lokasi

## SIMILARITY SCORE INTERPRETATION:
- > 0.8 = SANGAT YAKIN, langsung detail lengkap
- > 0.6 = YAKIN, langsung jawab dengan detail
- > 0.4 = CUKUP YAKIN, jawab dengan sedikit disclaimer "Sepertinya ini adalah..."
- > 0.3 = MUNGKIN, jawab dengan "Kemungkinan ini adalah..."
- < 0.3 = TIDAK YAKIN, baru tanya klarifikasi

Remember: Talk first, act during talking, explain what happened! GUNAKAN DATA RAG LANGSUNG!`;
}

  private async handleTurn(): Promise<LiveServerMessage[]> {
    const turn: LiveServerMessage[] = [];
    let done = false;
    
    console.log("‚è≥ Starting to handle turn, waiting for messages...");
    
    while (!done) {
      const message = await this.waitMessage();
      turn.push(message);
      
      if (message.serverContent && message.serverContent.turnComplete) {
        console.log('üèÅ Turn complete flag received.');
        done = true;
      }
    }
    
    console.log('‚úÖ Turn handling finished.', {
      messagesProcessed: turn.length,
      audioChunks: this.audioChunks.length
    });

    // Process audio after turn is complete
    if (this.audioChunks.length > 0) {
      this.processCompleteAudio();
    }
    
    return turn;
  }

    private async waitMessage(): Promise<LiveServerMessage> {
    let done = false;
    let message: LiveServerMessage | undefined = undefined;
    
    while (!done) {
      message = this.responseQueue.shift();
      if (message) {
        // CRITICAL: Process message here like documentation
        this.handleModelTurn(message);
        done = true;
      } else {
        await new Promise((resolve) => setTimeout(resolve, 100));
      }
    }
    return message!;
  }

  private processMessage(message: LiveServerMessage): { hasAudio: boolean; hasFunctionCalls: boolean; hasText: boolean } {
  let hasAudio = false;
  let hasFunctionCalls = false;
  let hasText = false;

  console.log('üì® Processing message:', {
    hasToolCall: !!message.toolCall,
    hasServerContent: !!message.serverContent,
    turnComplete: message.serverContent?.turnComplete
  });

  // 1. Handle function calls FIRST
  if (message.toolCall?.functionCalls && message.toolCall.functionCalls.length > 0) {
    console.log('üîß Function calls found:', message.toolCall.functionCalls.length);
    hasFunctionCalls = true;
    
    message.toolCall.functionCalls.forEach((call, index) => {
      console.log(`üéØ Executing function ${index + 1}:`, {
        name: call.name,
        args: call.args,
        id: call.id
      });
      
      if (call.name) {
        this.executeFunctionCall(call.name, call.args);
      }
    });

    // Send tool response immediately
    if (this.session) {
      const functionResponses = message.toolCall.functionCalls.map(call => ({
        id: call.id,
        name: call.name,
        response: { response: 'Function executed successfully' }
      }));
      
      console.log('üì§ Sending tool responses:', functionResponses);
      this.session.sendToolResponse({ functionResponses });
    }
  }

  // 2. Handle content (text and audio)
  if (message.serverContent?.modelTurn?.parts) {
    message.serverContent.modelTurn.parts.forEach((part, index) => {
      // Handle text content
      if (part?.text) {
        console.log('üí¨ Text content received:', part.text.substring(0, 200) + '...');
        hasText = true;
        
        // Send text to UI
        this.config.onTextGenerated?.(part.text);
      }

      // Handle audio content
      if (part?.inlineData?.mimeType?.includes('audio') && part.inlineData.data) {
        console.log(`üéµ Audio chunk ${index} received (${part.inlineData.data.length} chars)`);
        this.audioChunks.push(part.inlineData.data);
        hasAudio = true;
      }
    });
  }

  return { hasAudio, hasFunctionCalls, hasText };
}

  private handleModelTurn(message: LiveServerMessage) {
    console.log('üì® Processing message:', {
      hasToolCall: !!message.toolCall,
      hasServerContent: !!message.serverContent,
      turnComplete: message.serverContent?.turnComplete
    });

    // 1. Store function calls for later execution (MODIFIED)
    if (message.toolCall) {
      console.log('üîß Function calls found:', message.toolCall.functionCalls?.length || 0);
      
      message.toolCall.functionCalls?.forEach(functionCall => {
        console.log(`üìù Storing function call: ${functionCall.name}`, JSON.stringify(functionCall.args));
        
        // Store function call instead of executing immediately
        this.pendingFunctionCalls.push({
          name: functionCall.name ?? '',
          args: functionCall.args,
          id: functionCall.id ?? ''
        });
      });

      // Send tool response like before
      this.session?.sendToolResponse({
        functionResponses:
          message.toolCall.functionCalls?.map(functionCall => ({
            id: functionCall.id,
            name: functionCall.name,
            response: { response: 'Function queued for execution' } // Updated response
          })) ?? []
      });
    }

    // 2. Handle content (audio and text)
    if (message.serverContent?.modelTurn?.parts) {
      message.serverContent.modelTurn.parts.forEach((part, index) => {
        // Handle text content
        if (part?.text) {
          console.log('üí¨ Text content:', part.text.substring(0, 200) + '...');
          this.config.onTextGenerated?.(part.text);
        }

        // Handle audio content
        if (part?.inlineData) {
          console.log(`üéµ Audio chunk ${index} received:`, {
            mimeType: part.inlineData.mimeType,
            dataSize: part.inlineData.data?.length
          });
          
          this.audioChunks.push(part.inlineData.data ?? '');
        }
      });
    }
  }

  private createWavHeader(
    dataLength: number,
    options: { numChannels: number; sampleRate: number; bitsPerSample: number }
  ): Buffer {
    const { numChannels, sampleRate, bitsPerSample } = options;
    const byteRate = sampleRate * numChannels * bitsPerSample / 8;
    const blockAlign = numChannels * bitsPerSample / 8;
    const buffer = Buffer.alloc(44);

    buffer.write('RIFF', 0);                      // ChunkID
    buffer.writeUInt32LE(36 + dataLength, 4);     // ChunkSize
    buffer.write('WAVE', 8);                      // Format
    buffer.write('fmt ', 12);                     // Subchunk1ID
    buffer.writeUInt32LE(16, 16);                 // Subchunk1Size (PCM)
    buffer.writeUInt16LE(1, 20);                  // AudioFormat (1 = PCM)
    buffer.writeUInt16LE(numChannels, 22);        // NumChannels
    buffer.writeUInt32LE(sampleRate, 24);         // SampleRate
    buffer.writeUInt32LE(byteRate, 28);           // ByteRate
    buffer.writeUInt16LE(blockAlign, 32);         // BlockAlign
    buffer.writeUInt16LE(bitsPerSample, 34);      // BitsPerSample
    buffer.write('data', 36);                     // Subchunk2ID
    buffer.writeUInt32LE(dataLength, 40);         // Subchunk2Size

    return buffer;
  }

  private convertToWav(rawData: string[], mimeType: string): Buffer {
    const options = this.parseMimeType(mimeType);
    const dataLength = rawData.reduce((a, b) => a + b.length, 0);
    const wavHeader = this.createWavHeader(dataLength, options);
    const buffer = Buffer.concat(rawData.map(data => Buffer.from(data, 'base64')));

    return Buffer.concat([wavHeader, buffer]);
  }

  private parseMimeType(mimeType: string) {
    const [fileType, ...params] = mimeType.split(';').map(s => s.trim());
    const [, format] = fileType.split('/');

    const options = {
      numChannels: 1,
      bitsPerSample: 16,
      sampleRate: 24000,
    };

    if (format && format.startsWith('L')) {
      const bits = parseInt(format.slice(1), 10);
      if (!isNaN(bits)) {
        options.bitsPerSample = bits;
      }
    }

    for (const param of params) {
      const [key, value] = param.split('=').map(s => s.trim());
      if (key === 'rate') {
        options.sampleRate = parseInt(value, 10);
      }
    }

    return options;
  }

   private processCompleteAudio() {
    console.log(`üéµ Processing ${this.audioChunks.length} audio chunks...`);
    console.log(`üîß Processing ${this.pendingFunctionCalls.length} pending function calls...`);
    
    if (this.audioChunks.length === 0 && this.pendingFunctionCalls.length === 0) {
      console.log('‚ö†Ô∏è No audio chunks or function calls to process');
      return;
    }
    
    if (this.isPlayingAudio) {
      console.log('‚ö†Ô∏è Already playing audio, skipping processing');
      return;
    }

    try {
      // Execute pending function calls FIRST (ADDED)
      if (this.pendingFunctionCalls.length > 0) {
        console.log('üöÄ Executing queued function calls...');
        this.pendingFunctionCalls.forEach((functionCall, index) => {
          console.log(`üéØ Executing queued function ${index + 1}:`, {
            name: functionCall.name,
            args: functionCall.args,
            id: functionCall.id
          });
          
          this.executeFunctionCall(functionCall.name, functionCall.args);
        });
        
        // Clear pending function calls
        this.pendingFunctionCalls = [];
        console.log('‚úÖ All queued function calls executed');
      }

      // Process audio if available
      if (this.audioChunks.length > 0) {
        const combinedAudioData = this.audioChunks.join('');
        console.log('üîó Combined audio data size:', combinedAudioData.length);
        
        if (combinedAudioData.length === 0) {
          console.warn('‚ùå Combined audio data is empty');
          this.audioChunks = [];
          return;
        }

        const audioBuffer = this.convertToWav(this.audioChunks, 'audio/pcm;rate=24000');
        const audioBase64 = audioBuffer.toString('base64');
        
        console.log(`üîä Sending complete audio to UI (${audioBase64.length} chars)`);
        this.config.onAudioGenerated(audioBase64);
      }
      
    } catch (error) {
      console.error('‚ùå Error processing complete audio and functions:', error);
    } finally {
      this.audioChunks = [];
      this.pendingFunctionCalls = []; // Ensure cleanup
    }
  }

  private executeFunctionCall(functionName: string, args: any) {
    console.log(`üöÄ EXECUTING FUNCTION: ${functionName}`, {
      functionName,
      args,
      hasArgs: !!args
    });
    
    try {
      switch (functionName) {
        case 'show_location_details':
          console.log('üìç Calling onLocationDetails:', {
            locationId: args?.location_id,
            focusMap: args?.focus_map !== false
          });
          this.config.onLocationDetails(args?.location_id, args?.focus_map !== false);
          break;
          
        case 'navigate_to_location':
          console.log('üß≠ Calling onNavigate:', {
            locationId: args?.location_id,
            transportMode: args?.transport_mode || 'driving'
          });
          this.config.onNavigate(args?.location_id, args?.transport_mode || 'driving');
          break;
          
        case 'highlight_locations':
          console.log('üî¶ Calling onHighlightLocations:', {
            locationIds: args?.location_ids,
            highlightType: args?.highlight_type || 'pulse'
          });
          this.config.onHighlightLocations(args?.location_ids || [], args?.highlight_type || 'pulse');
          break;
          
        case 'set_map_filter':
          console.log('üîß Calling onSetMapFilter:', { filter: args?.filter });
          this.config.onSetMapFilter(args?.filter);
          break;
          
        case 'find_nearby_locations':
          console.log('üìç Calling onFindNearby:', {
            coordinates: args?.coordinates,
            radiusKm: args?.radius_km || 5,
            filterType: args?.filter_type
          });
          this.config.onFindNearby(args?.coordinates, args?.radius_km || 5, args?.filter_type);
          break;
          
        default:
          console.warn('‚ùå Unknown function:', functionName);
      }
      
      console.log(`‚úÖ Function ${functionName} executed successfully`);
      
    } catch (error) {
      console.error(`‚ùå Error executing function ${functionName}:`, error);
    }
  }

  private convertToCleanWav(base64Data: string): Buffer {
    try {
      const rawAudioData = Buffer.from(base64Data, 'base64');
      const sampleRate = 24000, numChannels = 1, bitsPerSample = 16;
      const byteRate = sampleRate * numChannels * (bitsPerSample / 8);
      const blockAlign = numChannels * (bitsPerSample / 8);
      const header = Buffer.alloc(44);

      header.write('RIFF', 0);
      header.writeUInt32LE(36 + rawAudioData.length, 4);
      header.write('WAVE', 8);
      header.write('fmt ', 12);
      header.writeUInt32LE(16, 16);
      header.writeUInt16LE(1, 20);
      header.writeUInt16LE(numChannels, 22);
      header.writeUInt32LE(sampleRate, 24);
      header.writeUInt32LE(byteRate, 28);
      header.writeUInt16LE(blockAlign, 32);
      header.writeUInt16LE(bitsPerSample, 34);
      header.write('data', 36);
      header.writeUInt32LE(rawAudioData.length, 40);

      let audioBuffer: Buffer = Buffer.concat([header, rawAudioData]);
      
      if (AudioProcessor?.normalizeAudioVolume) audioBuffer = AudioProcessor.normalizeAudioVolume(audioBuffer);
      if (AudioProcessor?.removeClicks) audioBuffer = AudioProcessor.removeClicks(audioBuffer);
      
      return audioBuffer;
    } catch (error) {
      console.error('Error converting audio:', error);
      throw error;
    }
  }

  get connected(): boolean { return this.isConnected; }
  get connecting(): boolean { return this.isConnecting; }
  get processing(): boolean { return this.isProcessing; }

  async reconnect(): Promise<void> {
    this.disconnect();
    await this.initialize();
  }

  disconnect() {
    this.isProcessing = false;
    this.isPlayingAudio = false;
    if (this.session) {
      try { this.session.close(); } catch (error) { console.warn('Error closing session:', error); }
      this.session = undefined;
    }
    this.isConnected = false;
    this.isConnecting = false;
    this.responseQueue = [];
    this.audioChunks = [];
    this.pendingFunctionCalls = []; // Add cleanup for pending function calls
  }
}